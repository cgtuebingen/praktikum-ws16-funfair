\documentclass{utue} %uumi.cls required for Uni corporate design
\usepackage{listings}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}
\usepackage{multirow,booktabs,setspace,caption}
\usepackage{subfig}

\lstset{numbers=left, numberstyle=\tiny, numbersep=5pt, xleftmargin=10mm}

% Values for title generation
\title{Funfair: EEG-based game control}
\author{Robert Eisele \& Robert Geirhos}
\date{\today}

% Subtitle is optional. It represents what kind of work you did.
\subtitle{Praktikum Computergrafik WS2016/17}

\begin{document}

% You can place a teaser as follows. (Otherwise, just uncomment the following part)
\teaser{
    \includegraphics[width=\textwidth]{images/teaser_funfair.png}
    \caption{You can place a teaser here.}
    \label{fig:teaser}
}     
%TODO idea: why not include a fancy animated video teaser here? That'd be beautiful...

% Creates title of document and additional title page.
\maketitle

\section*{Abstract [insg.: 6 Seiten, mind. 4 Seiten reiner Text]}

Electroencephalography (EEG) is a technique for measuring brain activity. Although widely used in neuroscientific research, it has thus far only rarely been used for the purpose of computer game control. Here we show that a leightweight EEG device, the Emotiv XXX, can be successfully used to play a broad range of simple yet challenging computer games. Furthermore, we demonstrate that rather than merely substituting a mouse or keyboard, an EEG device enables the development of a new type of games which go beyond traditional means of game control. We specifically developed six games, all of which can be played by controlling one's mental activity. We anticipate our work to be a starting point for ever more complex games and a new, keyboard-free gaming experience.


% Give an overview over your project here. Give a glimpse insight in the problem and write why it is important to be solved. Write what you did to make your implementation better than the state of the art implementations.


\section{Introduction}

Give an introduction to the problem. This might start from history, motivate the problem and end with the computational demands. Motivate why to use GPUs. You should also introduce CUDA in a way that makes an average computer science student understand what it is.

\section{Description of the Solution}

Describe your solution to the problem in detail. I propose to spend one chapter on a general, simple solution strategy so that the reader is familiar with how to solve the problem. This could also be a summary of the paper or document you mainly used to base your work on.

Subsequently, your particular solution should be described. Spend at least one chapter on describing the (parallel) improvements you made upon the naive implementation. It might be advantageous to split the implementation description to parts, each filling a subsection, and to give an overview over those parts first.

Key points to mention:

- Breite vor Schoenheit

- no "boring replacement of keyboard through EEG"



\subsection{Sensor data analysis}
All data, if not stated otherwise, were analyzed using R \cite{RCoreTeam}. Our goal was to build several sensor data classifier which, based on an analysis of the data, would be able to tell apart several menal states (\texttt{no\_action, action\_1, action\_2, ...}). In the process of building these classifiers, we faced two key challenges: Firstly, there was no prior mapping from the 16 sensors to a mental state - moreover, we did not even know which sensor would correspond to which mental signal (like $ \alpha $ or $ \beta $ waves). Secondly, the sensor data - as is the case for almost any recorded data - seemed to be noisy. We therefore approached both challenges not in a knowledge-driven, but rather in a data-driven way: We recorded data for a variety of different actions and mental commands (Fig. \ref{fig:sensor_values}) and carefully explored and visualized the sensor data recordings. As can be seen in the Figure, some commands such as \texttt{clench fist} were barely visible in the data, whereas other commands (e.g. \texttt{shake head, clench teeth}) produced considerably consistent spike-like patterns. We therefore decided to focus on the actions that seemed to be visually distinguishable in the data.

As most actions lasted for a very brief period of time, roughly 1-2 seconds, we buffered all sensor data individually with a sliding window of 2 seconds, which corresponds to 256 data points per sensor since the sampling frequency was approximately 128 Hz, and took this sliding window as the basis for all further data analysis. We then implemented two different types of classification. The first one is a mapping based on all sensors to a continuous value indicating the degree of relaxation (from tense, agitated to very relaxed and calm). It was achieved by taking into account the variance of the sum of all sensor data, which is based on the observation that a very tense state cannot be traced back to one particular sensor, but elicits high variance in several sensor recordings. The second type of classification, detecting discrete events, directly focuses on specific sensors: 
\texttt{Y} and \texttt{X} for \texttt{shake head} classification, \texttt{F3} for \texttt{nodding} and \texttt{O1} for the classification of \texttt{clench teeth}. It would have been possible to classify \texttt{squint eyes} as well, however the signals appeared to be hardly distinguishable from \texttt{clench teeth} (and furthermore closing one's eyes during gameplay was found to be rather annoying). This second type of classification was achieved through thresholding the windowed sample variance of a targeted sensor. As a result, we were able to classify one continuous mental state (\texttt{relaxation}) as well as three discrete events (\texttt{clench teeth, nodding, shake head}).

\subsection{Gyro data analysis}

\subsection{Architecture \& Backend}


\subsection{Highstriker}
A highstriker is a classic game that can be found on most funfairs. We took the core idea of striking with a hammer to achieve a score on a vertical bar and made a few adaptions to it. Rather than a using a virtual hammer, we mapped current the mental state to a score: the more relaxed one is, the better the score (and the friendlier the status indicator, ranging from \texttt{Burnout candidate} to \texttt{Master of Yoga}). As this game is, in our experience, both quite fun and very easy to play, we made it the first game, enabling the user to get to know how he can control his mental activity to achieve a good score. This then prepares for more complex games such as the balancing game. So-called \textit{brain self-regulation} based on visual feedback has been successfully used in clinical settings, e.g. to teach psychopaths to control their aggression \cite{Konicar2015}. It would be interesting to see whether some of our games (the ones that can only be won when one learns how to calm down and relax quickly) could be used to train children suffering from ADHD (attention deficit hyperactivity disorder) in a fun way.

\subsection{Painter}
The core idea behind the painter game can be described as follows: Wouldn't it be fun to take a snapshot of yourself (using your computer's camera) and then having this image painted in a style that matches your mood? 

We achieved this by accessing the computer's camera through the browser, sending the plain image to the backend which forwards it, along with a style inferred from the current mood, to the Turbo-Deepart website\footnote{\url{http://turbo.deepart.io/}} via the DeepArt API\footnote{\url{https://github.com/deepart-io/deepart-api}} to paint the image, which is then subsequently sent back to the game. DeepArt basically implements a neural network-based technique for painting an input image in the style of an arbitrary style image \cite{Gatys2016}. For an illustration of how the Lena image looks like when painted in four different styles, see Fig. \ref{fig:painter_styles}. We were able to contribute a commit to the DeepArt API (\texttt{rgeirhos}, commit \texttt{50f200f}, merged Dec 14, 2016), solving an early image retrieval issue.

As an additional feature, the painted image is not only shown but lies behind a veil first (low alpha value). One then needs to use either the mouse to hover over the image and reveal it piece by piece, or use a Leap Motion controller\footnote{\url{https://www.leapmotion.com/}} to do so. A Leap Motion device measures hand gestures and translates them into a 3D hand model on the screen. We mapped the position of the hand model to a certain position on the canvas, which enables the user to simulate holding a paint brush in his or her hands and paint the image with painting-like movements.

\begin{figure}
	\centering
	\subfloat{\includegraphics[width=0.22\columnwidth]{images/lena_6.jpg}}\hspace{0.02\columnwidth}
	\subfloat{\includegraphics[width=0.22\columnwidth]{images/lena_24.jpg}}\hspace{0.02\columnwidth}
	\subfloat{\includegraphics[width=0.22\columnwidth]{images/lena_26.jpg}}\hspace{0.02\columnwidth}
	\subfloat{\includegraphics[width=0.22\columnwidth]{images/lena_21.jpg}}
	\caption{Four different style examples for the painter game. Images are sorted from a relaxed, calm style (left) to a more excited, agitated style (right).}
	\label{fig:painter_styles}
\end{figure}

\subsection{Magic Duel}
\subsection{Mastermind}
Mastermind is a board game where one player selects a four-digit, order-sensitive color code and the other player tries to infer this code based on the feedback he receives in several rounds of guessing: Basically, for every token in the correct position (i.e. matching the code's color at this position), one receives a black feedback token; for every correctly colored token (that is, the color appears in the code) in the wrong position, one receives a white feedback token. This game serves as a demonstration that also games with four different control events can be controlled with the Emotiv device. The full list of game commands (in brackets: corresponding mental commands) is: change row (\texttt{nod}), change column (\texttt{shake head}), change color (\texttt{clench teeth}) and finally do nothing (every other mental state / action). In our experience, it is quite interesting to play this game with head movements and mental commands for the first time, however after a while, one would like to use a less cumbersome mouse instead. As this was not the case for other games, we think this might point to the fact that using an EEG device to replace a mouse is a bit boring - after all, we are pretty good in controlling computers with mouse and keyboard on a daily basis, whereas games that can only be played with such a device seem to be way more interesting.


\subsection{Wireloop}
\subsection{Balancing}
Our balancing game is perhaps the most challenging game, as it requires to balance a little man on an inherently instable seesaw: if tilted to one side, one needs to strain oneself in order to get it back to balance - but then, it tilts to the other side and one instantly needs to switch to 'zen mode', to total relaxation... if one is able to do this for 30 seconds, the game is won, otherwise it is re-started.

In some relaxation techniques such as \textit{Progressive Relaxation} \cite{Jacobson1938}, being able to consciously change from straining oneself to relaxation mode (and vice versa) is a core element: if one learns and trains how to change between these states, this can be effortlessly applied in situations where one is naturally stressed and tense, such as ahead of an exam or a job interview. It would be interesting to explore whether a game like our balancing game could be used in a similar fashion - which would then enable one to teach relaxation techniques to a broader variety of people than those attracted by, say, adult evening classes.




\begin{figure*}[h!]
	\centering
	\includegraphics[width=1.0\textwidth]{images/sensor_values.png}
	\caption{Raw sensor data for ten different actions and mental commands. All data are normalized to lie within [0, 1]. Every sensor has a different offset here for better visibility. Sensors from top to bottom: \texttt{Y, X, F4, FC6, AF4, F8, T8, P8, O2, O1, P7, T7, F7, AF3, FC5, F3}. Every action corresponds to one minute of data recording, during which over the first 45 seconds the action was briefly executed every five seconds, followed by a short break until the next type of action began.}
	\label{fig:sensor_values}
\end{figure*}

\section{Possible Extensions}

Describe possible extensions and discuss why they could be useful.

Viele Daten -> LSTM

weitere Spiele beliebig hinzufuegbar

Spiele fancier machen (3D, ...)


\appendix

\section{General Infos}

You don't need an appendix. The two appendix sections are just there to give some additional or general information.

This document describes roughly, what the documentation of the Praktikum project should look like.

Note that your final documentation of your project should contain 6 text pages using this template plus the cover and the empty sheet at the beginning. Within the 6 pages, at least 4 pages should be only text.


\bibliographystyle{alpha}
\bibliography{bibliography}

\end{document}

